{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a9a3ce8-180a-4457-9227-1e26639d0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "from html import unescape\n",
    "import subprocess\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871b5bd-6eea-439f-b343-188e8f2bbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects Mermaid Diagrams from Stack Overflow\n",
    "def search_stackoverflow_mermaid_diagrams(query=\"mermaid\", pages=5):\n",
    "    base_url = \"https://api.stackexchange.com/2.3/search/advanced\"\n",
    "    diagrams = []\n",
    "    headers = {\"User-Agent\": \"mermaid-diagram-collector\"}\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        print(f\" Searching page {page}...\")\n",
    "        params = {\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"relevance\",\n",
    "            \"q\": query,\n",
    "            \"site\": \"stackoverflow\",\n",
    "            \"filter\": \"withbody\",\n",
    "            \"pagesize\": 20,\n",
    "            \"page\": page,\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        for item in data.get(\"items\", []):\n",
    "            body = unescape(item[\"body\"])\n",
    "            matches = re.findall(r\"```mermaid(.*?)```\", body, re.DOTALL)\n",
    "            for match in matches:\n",
    "                diagrams.append(match.strip())\n",
    "\n",
    "    return diagrams\n",
    "\n",
    "def save_diagrams(diagrams, output_dir=\"so_mermaid_diagrams\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i, d in enumerate(diagrams):\n",
    "        with open(os.path.join(output_dir, f\"diagram_{i+1}.mmd\"), \"w\") as f:\n",
    "            f.write(\"```mermaid\\n\" + d + \"\\n```\")\n",
    "    print(f\"\\n Saved {len(diagrams)} Mermaid diagrams to '{output_dir}/'\")\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    diagrams = search_stackoverflow_mermaid_diagrams(query=\"mermaid\", pages=5)\n",
    "    save_diagrams(diagrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2218c82-81ac-4920-830e-3f2fd75d52b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Searching page 1...\n",
      " Searching page 2...\n",
      " Searching page 3...\n",
      " Searching page 4...\n",
      " Searching page 5...\n",
      " Searching page 6...\n",
      " Searching page 7...\n",
      " Searching page 8...\n",
      " Searching page 9...\n",
      " Searching page 10...\n",
      " Searching page 11...\n",
      " Searching page 12...\n",
      " Searching page 13...\n",
      " Searching page 14...\n",
      " Searching page 15...\n",
      " Searching page 16...\n",
      " Searching page 17...\n",
      " Searching page 18...\n",
      " Searching page 19...\n",
      " Searching page 20...\n",
      " Searching page 21...\n",
      " Searching page 22...\n",
      " Searching page 23...\n",
      " Searching page 24...\n",
      " Searching page 25...\n",
      " Searching page 26...\n",
      " Searching page 27...\n",
      " Searching page 28...\n",
      " Searching page 29...\n",
      " Searching page 30...\n",
      " Searching page 31...\n",
      " Searching page 32...\n",
      " Searching page 33...\n",
      " Searching page 34...\n",
      " Searching page 35...\n",
      " Searching page 36...\n",
      " Searching page 37...\n",
      " Searching page 38...\n",
      " Searching page 39...\n",
      " Searching page 40...\n",
      " Searching page 41...\n",
      " Searching page 42...\n",
      " Searching page 43...\n",
      " Searching page 44...\n",
      " Searching page 45...\n",
      " Searching page 46...\n",
      " Searching page 47...\n",
      " Searching page 48...\n",
      " Searching page 49...\n",
      " Searching page 50...\n",
      " Searching page 51...\n",
      " Searching page 52...\n",
      " Searching page 53...\n",
      " Searching page 54...\n",
      " Searching page 55...\n",
      " Searching page 56...\n",
      " Searching page 57...\n",
      " Searching page 58...\n",
      " Searching page 59...\n",
      " Searching page 60...\n",
      " Searching page 61...\n",
      " Searching page 62...\n",
      " Searching page 63...\n",
      " Searching page 64...\n",
      " Searching page 65...\n",
      " Searching page 66...\n",
      " Searching page 67...\n",
      " Searching page 68...\n",
      " Searching page 69...\n",
      " Searching page 70...\n",
      " Searching page 71...\n",
      " Searching page 72...\n",
      " Searching page 73...\n",
      " Searching page 74...\n",
      " Searching page 75...\n",
      " Searching page 76...\n",
      " Searching page 77...\n",
      " Searching page 78...\n",
      " Searching page 79...\n",
      " Searching page 80...\n",
      " Searching page 81...\n",
      " Searching page 82...\n",
      " Searching page 83...\n",
      " Searching page 84...\n",
      " Searching page 85...\n",
      " Searching page 86...\n",
      " Searching page 87...\n",
      " Searching page 88...\n",
      " Searching page 89...\n",
      " Searching page 90...\n",
      " Searching page 91...\n",
      " Searching page 92...\n",
      " Searching page 93...\n",
      " Searching page 94...\n",
      " Searching page 95...\n",
      " Searching page 96...\n",
      " Searching page 97...\n",
      " Searching page 98...\n",
      " Searching page 99...\n",
      " Searching page 100...\n",
      "\n",
      " Saved 4 data model Mermaid diagrams to 'so_mermaid_data_models/'\n"
     ]
    }
   ],
   "source": [
    "# Collects Mermaid ER and Class Diagrams from Stack Overflow\n",
    "def search_stackoverflow_mermaid_diagrams(query=\"mermaid\", pages=100):\n",
    "    base_url = \"https://api.stackexchange.com/2.3/search/advanced\"\n",
    "    diagrams = []\n",
    "    headers = {\"User-Agent\": \"mermaid-diagram-collector\"}\n",
    "\n",
    "    data_model_keywords = [\"erDiagram\", \"classDiagram\"]\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        print(f\" Searching page {page}...\")\n",
    "        params = {\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"relevance\",\n",
    "            \"q\": query,\n",
    "            \"site\": \"stackoverflow\",\n",
    "            \"filter\": \"withbody\",\n",
    "            \"pagesize\": 20,\n",
    "            \"page\": page,\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        for item in data.get(\"items\", []):\n",
    "            body = unescape(item[\"body\"])\n",
    "            matches = re.findall(r\"```mermaid(.*?)```\", body, re.DOTALL)\n",
    "            for match in matches:\n",
    "                stripped = match.strip()\n",
    "                if any(kw in stripped for kw in data_model_keywords):\n",
    "                    diagrams.append(stripped)\n",
    "\n",
    "    return diagrams\n",
    "\n",
    "def save_diagrams(diagrams, output_dir=\"so_mermaid_data_models\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i, d in enumerate(diagrams):\n",
    "        with open(os.path.join(output_dir, f\"diagram_{i+1}.mmd\"), \"w\") as f:\n",
    "            f.write(\"```mermaid\\n\" + d + \"\\n```\")\n",
    "    print(f\"\\n Saved {len(diagrams)} data model Mermaid diagrams to '{output_dir}/'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    diagrams = search_stackoverflow_mermaid_diagrams(query=\"mermaid\", pages=100)\n",
    "    save_diagrams(diagrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0309e61-9149-4c5c-8103-79b443005cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Searching page 1...\n",
      " Searching page 2...\n",
      " Searching page 3...\n",
      " Searching page 4...\n",
      " Searching page 5...\n",
      " Searching page 6...\n",
      " Searching page 7...\n",
      " Searching page 8...\n",
      " Searching page 9...\n",
      " Searching page 10...\n",
      " Searching page 11...\n",
      " Searching page 12...\n",
      " Searching page 13...\n",
      " Searching page 14...\n",
      " Searching page 15...\n",
      " Searching page 16...\n",
      " Searching page 17...\n",
      " Searching page 18...\n",
      " Searching page 19...\n",
      " Searching page 20...\n",
      " Searching page 21...\n",
      " Searching page 22...\n",
      " Searching page 23...\n",
      " Searching page 24...\n",
      " Searching page 25...\n",
      " Searching page 26...\n",
      " Searching page 27...\n",
      " Searching page 28...\n",
      " Searching page 29...\n",
      " Searching page 30...\n",
      " Searching page 31...\n",
      " Searching page 32...\n",
      " Searching page 33...\n",
      " Searching page 34...\n",
      " Searching page 35...\n",
      " Searching page 36...\n",
      " Searching page 37...\n",
      " Searching page 38...\n",
      " Searching page 39...\n",
      " Searching page 40...\n",
      " Searching page 41...\n",
      " Searching page 42...\n",
      " Searching page 43...\n",
      " Searching page 44...\n",
      " Searching page 45...\n",
      " Searching page 46...\n",
      " Searching page 47...\n",
      " Searching page 48...\n",
      " Searching page 49...\n",
      " Searching page 50...\n",
      "\n",
      " Saved 3 PlantUML data model diagrams to 'so_plantuml_data_models/'\n"
     ]
    }
   ],
   "source": [
    "# Collects PlantUML Diagrams from Stack Overflow\n",
    "def search_stackoverflow_plantuml_diagrams(query=\"plantuml\", pages=50):\n",
    "    base_url = \"https://api.stackexchange.com/2.3/search/advanced\"\n",
    "    diagrams = []\n",
    "    headers = {\"User-Agent\": \"plantuml-diagram-collector\"}\n",
    "\n",
    "    data_model_keywords = [\"entity\", \"class\", \"database\", \"table\"]\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        print(f\" Searching page {page}...\")\n",
    "        params = {\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"relevance\",\n",
    "            \"q\": query,\n",
    "            \"site\": \"stackoverflow\",\n",
    "            \"filter\": \"withbody\",\n",
    "            \"pagesize\": 20,\n",
    "            \"page\": page,\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        for item in data.get(\"items\", []):\n",
    "            body = unescape(item[\"body\"])\n",
    "            matches = re.findall(r\"```(?:plantuml)?(.*?)```\", body, re.DOTALL | re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                stripped = match.strip()\n",
    "                if \"@startuml\" in stripped.lower() and any(kw in stripped.lower() for kw in data_model_keywords):\n",
    "                    diagrams.append(stripped)\n",
    "\n",
    "    return diagrams\n",
    "\n",
    "def save_diagrams(diagrams, output_dir=\"so_plantuml_data_models\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i, d in enumerate(diagrams):\n",
    "        with open(os.path.join(output_dir, f\"diagram_{i+1}.puml\"), \"w\") as f:\n",
    "            f.write(d)\n",
    "    print(f\"\\n Saved {len(diagrams)} PlantUML data model diagrams to '{output_dir}/'\")\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    diagrams = search_stackoverflow_plantuml_diagrams(query=\"plantuml\", pages=50)\n",
    "    save_diagrams(diagrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0985eebf-b154-42b3-815c-8d22648c8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ANSI escape sequences (like \\x1b[1m or \\033[0;31m)\n",
    "def strip_ansi_codes(text):\n",
    "    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "    return ansi_escape.sub('', text)\n",
    "\n",
    "# Searches GitHub for Mermaid Data Model Diagrams with 'erDiagram'\n",
    "def run_gh_search_mermaid(output_file=\"mermaid_results.json\", limit=50):\n",
    "    cmd = [\n",
    "        \"gh\", \"search\", \"code\",\n",
    "        \"erDiagram\",\n",
    "        \"--limit\", str(limit),\n",
    "        \"--json\", \"repository,path,url\"\n",
    "        \n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, env={**os.environ, \"NO_COLOR\": \"1\"})\n",
    "    if result.stderr:\n",
    "        print(\"Error:\", result.stderr)\n",
    "    \n",
    "    clean_output = strip_ansi_codes(result.stdout).strip()\n",
    "    if not clean_output:\n",
    "        print(\"GitHub search returned no results\")\n",
    "    else:\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(clean_output)\n",
    "        print(f\"Saved Mermaid GitHub search results to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Searches Github for PlantUML Diagrams \n",
    "def run_gh_search_plantuml(output_file=\"plantuml_results.json\", limit=50):\n",
    "    cmd = [\n",
    "        \"gh\", \"search\", \"code\",\n",
    "        \"@startuml\",\n",
    "        \"--limit\", str(limit),\n",
    "        \"--json\", \"repository,path,url\"\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, env={**os.environ, \"NO_COLOR\": \"1\"})\n",
    "    if result.stderr:\n",
    "        print(\"Error:\", result.stderr)\n",
    "    clean_output = strip_ansi_codes(result.stdout).strip()\n",
    "    if not clean_output:\n",
    "        print(\"GitHub search returned no results\")\n",
    "    else:\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(clean_output)\n",
    "        print(f\"PlantUML GitHub search results to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "409e5fcb-24a3-450a-bfbd-f978a904ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads Diagrams and Saves to Directory\n",
    "def download_diagrams_from_gh(json_file, output_dir, must_contain=None):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    saved = 0\n",
    "    for i, item in enumerate(data):\n",
    "        repo = item[\"repository\"][\"nameWithOwner\"]\n",
    "        path = item[\"path\"]\n",
    "        raw_url = f\"https://raw.githubusercontent.com/{repo}/HEAD/{path}\"  # HEAD handles main/master\n",
    "\n",
    "        try:\n",
    "            response = requests.get(raw_url)\n",
    "            content = response.text\n",
    "\n",
    "            if must_contain:\n",
    "                if not any(keyword in content for keyword in must_contain):\n",
    "                    continue\n",
    "\n",
    "            filename = os.path.join(output_dir, f\"diagram_{i+1}.txt\")\n",
    "            with open(filename, \"w\") as f_out:\n",
    "                f_out.write(content)\n",
    "            saved += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {raw_url}: {e}\")\n",
    "\n",
    "    print(f\"\\nDownloaded and saved {saved} diagram files to '{output_dir}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a87e3a7-b94e-49ed-8d48-c9f6954ee590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Mermaid GitHub search results to mermaid_results.json\n",
      "PlantUML GitHub search results to plantuml_results.json\n",
      "\n",
      "Downloaded and saved 88 diagram files to 'gh_mermaid_data_models'\n",
      "\n",
      "Downloaded and saved 69 diagram files to 'gh_plantuml_data_models'\n"
     ]
    }
   ],
   "source": [
    "# Run GitHub searches\n",
    "run_gh_search_mermaid(limit=100)\n",
    "run_gh_search_plantuml(limit=100)\n",
    "\n",
    "# Download and filter data model diagrams\n",
    "download_diagrams_from_gh(\n",
    "    json_file=\"mermaid_results.json\",\n",
    "    output_dir=\"gh_mermaid_data_models\",\n",
    "    must_contain=[\"erDiagram\", \"classDiagram\"]\n",
    ")\n",
    "\n",
    "download_diagrams_from_gh(\n",
    "    json_file=\"plantuml_results.json\",\n",
    "    output_dir=\"gh_plantuml_data_models\",\n",
    "    must_contain=[\"entity\", \"class\", \"database\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c19b1-c3f0-431a-9be2-63d6c4978503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
