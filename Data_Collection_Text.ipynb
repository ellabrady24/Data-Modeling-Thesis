{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a9a3ce8-180a-4457-9227-1e26639d0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "from html import unescape\n",
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b066c10-0ad2-486f-9b53-2de5f87e7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ANSI escape sequences (like \\x1b[1m or \\033[0;31m)\n",
    "def strip_ansi_codes(text):\n",
    "    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "    return ansi_escape.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "409e5fcb-24a3-450a-bfbd-f978a904ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads Diagrams and Saves to Directory\n",
    "def download_diagrams_from_gh(json_file, output_dir, must_contain=None):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    saved = 0\n",
    "    for i, item in enumerate(data):\n",
    "        repo = item[\"repository\"][\"nameWithOwner\"]\n",
    "        path = item[\"path\"]\n",
    "        raw_url = f\"https://raw.githubusercontent.com/{repo}/HEAD/{path}\"  # HEAD handles main/master\n",
    "\n",
    "        try:\n",
    "            response = requests.get(raw_url)\n",
    "            content = response.text\n",
    "\n",
    "            if must_contain:\n",
    "                if not any(keyword in content for keyword in must_contain):\n",
    "                    continue\n",
    "\n",
    "            filename = os.path.join(output_dir, f\"diagram_{i+1}.txt\")\n",
    "            with open(filename, \"w\") as f_out:\n",
    "                f_out.write(content)\n",
    "            saved += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {raw_url}: {e}\")\n",
    "\n",
    "    print(f\"\\nDownloaded and saved {saved} diagram files to '{output_dir}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed3c19b1-c3f0-431a-9be2-63d6c4978503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run gh search\n",
    "def run_gh_search(query, output_file=\"results.json\", total_limit=1000, batch_size=100):\n",
    "    all_results = []\n",
    "\n",
    "    for batch_start in range(0, total_limit, batch_size):\n",
    "        cmd = [\n",
    "            \"gh\", \"search\", \"code\",\n",
    "            query,\n",
    "            \"--limit\", str(batch_size),\n",
    "            \"--json\", \"repository,path,url\"\n",
    "        ]\n",
    "\n",
    "        print(f\"Running batch starting at {batch_start}...\")\n",
    "\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, env={**os.environ, \"NO_COLOR\": \"1\"})\n",
    "        if result.stderr:\n",
    "            print(\"Error:\", result.stderr)\n",
    "\n",
    "        clean_output = strip_ansi_codes(result.stdout).strip()\n",
    "\n",
    "        if not clean_output:\n",
    "            print(\"GitHub search returned no results for this batch\")\n",
    "            continue\n",
    "\n",
    "        batch_data = json.loads(clean_output)\n",
    "        if not batch_data:\n",
    "            print(\"No more data returned by GitHub.\")\n",
    "            break\n",
    "\n",
    "        all_results.extend(batch_data)\n",
    "\n",
    "        print(f\"Fetched {len(batch_data)} items in batch {batch_start}. Waiting to avoid rate limits...\")\n",
    "        time.sleep(65)\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"No results found at all.\")\n",
    "    else:\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(all_results, f, indent=2)\n",
    "        print(f\"Saved {len(all_results)} GitHub search results to {output_file}\")\n",
    "\n",
    "# Wrapper for Mermaid diagrams\n",
    "def run_gh_search_mermaid(output_file=\"mermaid_results.json\", total_limit=1000, batch_size=100):\n",
    "    run_gh_search(\"erDiagram\", output_file=output_file, total_limit=total_limit, batch_size=batch_size)\n",
    "\n",
    "# Wrapper for PlantUML diagrams\n",
    "def run_gh_search_plantuml(output_file=\"plantuml_results.json\", total_limit=1000, batch_size=100):\n",
    "    run_gh_search(\"@startuml\", output_file=output_file, total_limit=total_limit, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbc46611-e7e1-4bdd-a140-2ce050a62b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch starting at 0...\n",
      "Fetched 100 items in batch 0. Waiting to avoid rate limits...\n",
      "Running batch starting at 100...\n",
      "Fetched 100 items in batch 100. Waiting to avoid rate limits...\n",
      "Running batch starting at 200...\n",
      "Fetched 100 items in batch 200. Waiting to avoid rate limits...\n",
      "Running batch starting at 300...\n",
      "Fetched 100 items in batch 300. Waiting to avoid rate limits...\n",
      "Running batch starting at 400...\n",
      "Fetched 100 items in batch 400. Waiting to avoid rate limits...\n",
      "Running batch starting at 500...\n",
      "Fetched 100 items in batch 500. Waiting to avoid rate limits...\n",
      "Running batch starting at 600...\n",
      "Fetched 100 items in batch 600. Waiting to avoid rate limits...\n",
      "Running batch starting at 700...\n",
      "Fetched 100 items in batch 700. Waiting to avoid rate limits...\n",
      "Running batch starting at 800...\n",
      "Fetched 100 items in batch 800. Waiting to avoid rate limits...\n",
      "Running batch starting at 900...\n",
      "Fetched 100 items in batch 900. Waiting to avoid rate limits...\n",
      "Running batch starting at 1000...\n",
      "Fetched 100 items in batch 1000. Waiting to avoid rate limits...\n",
      "Running batch starting at 1100...\n",
      "Fetched 100 items in batch 1100. Waiting to avoid rate limits...\n",
      "Running batch starting at 1200...\n",
      "Fetched 100 items in batch 1200. Waiting to avoid rate limits...\n",
      "Running batch starting at 1300...\n",
      "Fetched 100 items in batch 1300. Waiting to avoid rate limits...\n",
      "Running batch starting at 1400...\n",
      "Fetched 100 items in batch 1400. Waiting to avoid rate limits...\n",
      "Saved 1500 GitHub search results to mermaid_results.json\n",
      "\n",
      "Downloaded and saved 1395 diagram files to 'gh_mermaid_data_models'\n"
     ]
    }
   ],
   "source": [
    "# Run search for mermaid and plantUML diagrams\n",
    "run_gh_search_mermaid(total_limit=1500, batch_size=100)\n",
    "run_gh_search_plantuml(total_limit=1500, batch_size=100)\n",
    "\n",
    "download_diagrams_from_gh(\n",
    "    json_file=\"mermaid_results.json\",\n",
    "    output_dir=\"gh_mermaid_data_models\",\n",
    "    must_contain=[\"erDiagram\", \"classDiagram\"]\n",
    ")\n",
    "\n",
    "download_diagrams_from_gh(\n",
    "    json_file=\"plantuml_results.json\",\n",
    "    output_dir=\"gh_plantuml_data_models\",\n",
    "    must_contain=[\"entity\", \"class\", \"database\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713d9058-97b8-423c-94ad-8b9c8f9e596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract code blocks from mermaid diagram files\n",
    "def extract_mermaid_blocks(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # For fenced blocks first\n",
    "    matches = re.findall(r\"```mermaid\\s*(.*?)```\", text, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches\n",
    "\n",
    "    # For raw mermaid code\n",
    "    if \"erDiagram\" in text or \"classDiagram\" in text:\n",
    "        return [text]\n",
    "\n",
    "    return []\n",
    "\n",
    "# Extract code blocks from plantUML diagram files\n",
    "def extract_plantuml_blocks(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Extract all plantUML blocks\n",
    "    all_blocks = re.findall(r\"@startuml\\s*(.*?)\\s*@enduml\", text, re.DOTALL)\n",
    "\n",
    "    # Keep only data models \n",
    "    data_model_blocks = []\n",
    "    for block in all_blocks:\n",
    "        lower_block = block.lower()\n",
    "\n",
    "        if any(token in lower_block for token in [\n",
    "            '||--', '|o--', 'o--o{', 'class ', 'entity', '{', '}'\n",
    "        ]):\n",
    "            # Must have no method definitions (brackets)\n",
    "            if not re.search(r'\\w+\\s*\\(.*?\\)', block):\n",
    "                data_model_blocks.append(block.strip())\n",
    "\n",
    "    return data_model_blocks\n",
    "\n",
    "diagrams = []\n",
    "mermaid_path = 'gh_mermaid_data_models'\n",
    "plantuml_path = 'gh_plantuml_data_models'\n",
    "\n",
    "# Process mermaid files\n",
    "for filename in os.listdir(mermaid_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(mermaid_path, filename)\n",
    "        blocks = extract_mermaid_blocks(file_path)\n",
    "        for block in blocks:\n",
    "            lines = [line.strip().lower() for line in block.strip().splitlines() if line.strip()]\n",
    "            if lines and (lines[0].startswith(\"erdiagram\") or lines[0].startswith(\"classdiagram\")):\n",
    "                diagrams.append({\n",
    "                    'filename': f\"mermaid_{filename}\",\n",
    "                    'file_path': file_path,\n",
    "                    'diagram_code': block,\n",
    "                    'diagram_type': 'mermaid'\n",
    "                })\n",
    "\n",
    "# Process plantUML files\n",
    "for filename in os.listdir(plantuml_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(plantuml_path, filename)\n",
    "        blocks = extract_plantuml_blocks(file_path)\n",
    "        for block in blocks:\n",
    "            diagrams.append({\n",
    "                'filename': f\"plantuml_{filename}\",\n",
    "                'file_path': file_path,\n",
    "                'diagram_code': block,\n",
    "                'diagram_type': 'plantuml'\n",
    "            })\n",
    "\n",
    "mermaid_dir = \"diagrams/mermaid_1\"\n",
    "plantuml_dir = \"diagrams/plantuml_1\"\n",
    "os.makedirs(mermaid_dir, exist_ok=True)\n",
    "os.makedirs(plantuml_dir, exist_ok=True)\n",
    "\n",
    "# Save diagrams as mermaid/plantUML files\n",
    "for i, diagram in enumerate(diagrams):\n",
    "    ext = \".mmd\" if diagram[\"diagram_type\"] == \"mermaid\" else \".puml\"\n",
    "    safe_name = diagram[\"filename\"].replace(\".txt\", \"\").replace(\"/\", \"_\")\n",
    "    dir_path = mermaid_dir if diagram[\"diagram_type\"] == \"mermaid\" else plantuml_dir\n",
    "    file_name = f\"{i:04d}_{safe_name}{ext}\"\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if diagram[\"diagram_type\"] == \"plantuml\":\n",
    "            f.write(f\"@startuml\\n{diagram['diagram_code'].strip()}\\n@enduml\")\n",
    "        else:\n",
    "            f.write(diagram[\"diagram_code\"].strip())\n",
    "\n",
    "# Save meta data to dataframe\n",
    "df = pd.DataFrame(diagrams)\n",
    "df.to_csv(\"diagrams/diagram_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6309f-d7e3-4742-a1eb-71df178e5aff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
